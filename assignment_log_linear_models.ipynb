{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "assignment_log_linear_models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jolee1101/test-repo/blob/master/assignment_log_linear_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XC5ASXZ8XDH"
      },
      "source": [
        "import random\n",
        "import sys\n",
        "from typing import Callable, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eq3FXdDzuYmF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSVp1VBXuY23"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PayJ9rfy8bvU"
      },
      "source": [
        "class LogLinearModel:\n",
        "    def __init__(\n",
        "        self,\n",
        "        feature_function: Callable,\n",
        "        learning_rate: float,\n",
        "        iterations: int,\n",
        "        loss: Callable,\n",
        "        gradient_loss: Callable,\n",
        "        verbose: bool,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ---\n",
        "        feature_function : Callable\n",
        "            Feature function mapping from X x Y -> R^m\n",
        "        learning_rate : float\n",
        "            Learning rate parameter eta for gradient descent\n",
        "        iterations : int\n",
        "            Number of iterations to run gradient descent for during `fit`\n",
        "        loss : Callable\n",
        "            Loss function to be used by this LogLinearModel instance as\n",
        "            a function of the parameters and the data X and y\n",
        "        gradient_loss : Callable\n",
        "            Closed form gradient of the `loss` function used for gradient descent as\n",
        "            a function of the parameters and the data X and y\n",
        "        verbose : bool\n",
        "            Verbosity level of the class. If verbose == True,\n",
        "            the class will print updates about the gradient\n",
        "            descent steps during `fit`\n",
        "\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        pass\n",
        "\n",
        "    def gradient_descent(self, X: np.ndarray, y: np.ndarray):\n",
        "        \"\"\"Performs one gradient descent step, and update parameters inplace.\n",
        "\n",
        "        Parameters\n",
        "        ---\n",
        "        X : np.ndarray\n",
        "            Data matrix\n",
        "        y : np.ndarray\n",
        "            Binary target values\n",
        "\n",
        "        Returns\n",
        "        ---\n",
        "        None\n",
        "\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        pass\n",
        "\n",
        "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
        "        \"\"\"Fits LogLinearModel class using gradient descent.\n",
        "\n",
        "        Parameters\n",
        "        ---\n",
        "        X : np.ndarray\n",
        "            Input data matrix\n",
        "        y : np.ndarray\n",
        "            Binary target values\n",
        "\n",
        "        Returns\n",
        "        ---\n",
        "        None\n",
        "\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        pass\n",
        "\n",
        "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Predicts binary target labels for input data `X`.\n",
        "\n",
        "        Parameters\n",
        "        ---\n",
        "        X : np.ndarray\n",
        "            Input data matrix\n",
        "\n",
        "        Returns\n",
        "        ---\n",
        "        np.ndarray\n",
        "            Predicted binary target labels\n",
        "\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3Ayb94i8Xmq"
      },
      "source": [
        "# Set seeds to ensure reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke2ULXNz8gvu"
      },
      "source": [
        "lr = LogisticRegression()\n",
        "\n",
        "llm = LogLinearModel(\n",
        "    feature_function=feature_function,  # TODO\n",
        "    learning_rate=learning_rate,  # Make sure that the model converges\n",
        "                                  # with your chosen learning rate\n",
        "    iterations=100,\n",
        "    loss=negative_log_likelihood,  # TODO\n",
        "    gradient_loss=gradient_negative_log_likelihood,  # TODO\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6Vpa6A78oNs"
      },
      "source": [
        "# First dataset\n",
        "# Fit both `lr` and your `llm` on this dataset and compare\n",
        "# the aspects described in the assignment PDF\n",
        "X, y = make_classification(\n",
        "    n_samples=100, random_state=42, n_informative=20, n_features=20, n_redundant=0\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7bQ0Zct8rQC"
      },
      "source": [
        "# Second dataset\n",
        "# Fit both `lr` and your `llm` on this dataset and compare\n",
        "# the aspects described in the assignment PDF\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    random_state=42,\n",
        "    n_informative=20,\n",
        "    n_redundant=10,\n",
        "    n_features=35,\n",
        "    n_repeated=5,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iThAvCNa8sWB"
      },
      "source": [
        "# Third dataset\n",
        "# Fit both `lr` and your `llm` on this dataset and compare\n",
        "# the aspects described in the assignment PDF\n",
        "X, y = make_classification(\n",
        "    n_samples=10000, random_state=42, n_informative=2, n_repeated=5\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3k7InTb8t-8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}